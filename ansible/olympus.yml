---
- name: fact and backend gathering
  hosts: all
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  tasks:
    - name: Gather facts from ALL hosts (regardless of limit or tags)
      ansible.builtin.setup:
      tags:
        - always

    - name: install packages only when the apt process is not locked
      ansible.builtin.apt:
        name: "{{ item }}"
        state: present
        update_cache: true
        cache_valid_time: 3600
        autoremove: true
        autoclean: true
      register: apt_action
      retries: 100
      until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
      loop:
        - python3-pip
        - net-tools
        - dmidecode
      tags:
        - always

    - name: ensure pip packages are installed
      ansible.builtin.pip:
        name: boto3
        executable: pip3
        umask: "0022"
      tags:
        - always

    - name: gathering terraform outputs
      terraform_backend_info:
        bucket: "{{ aws_bucket }}"
        object: "olympus-v2"
        aws_access_key: "{{ aws_access_key }}"
        aws_secret_access_key: "{{ aws_secret_access_key }}"
        aws_region: "{{ aws_region }}"
      become: false
      register: "olympus_v2"
      check_mode: false
      tags:
        - always
        - terraform_backend_info

    - name: set facts from backend info
      ansible.builtin.set_fact:
        _olympus_v2: "{{ olympus_v2['vars'] }}"
      check_mode: false
      tags:
        - always

    - name: print terraform outputs
      ansible.builtin.copy:
        content: "{{ _olympus_v2 | to_nice_json }}"
        dest: "./backend_info/{{ inventory_hostname }}.json"
        mode: "0744"
      when: print_terraform_backend
      delegate_to: localhost
      become: false
      check_mode: false
      tags:
        - terraform_backend_info

    - name: show terraform outputs
      ansible.builtin.debug:
        var: olympus_v2
      when: terraform_backend_debug is defined and terraform_backend_debug
      check_mode: false
      tags:
        - terraform_backend_info

- name: host preparation
  hosts: all
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  pre_tasks:
    - name: set _arch fact
      ansible.builtin.set_fact:
        _arch: "{%- if ansible_facts['architecture'] == 'aarch64' %}arm64{%- else %}amd64{%- endif %}"
      tags:
        - always

    - name: set _system fact
      ansible.builtin.set_fact:
        _system: "{{ ansible_facts['system']|lower }}"
      tags:
        - always

    - name: add authorized keys
      ansible.posix.authorized_key:
        user: "{{ item }}"
        state: present
        key: "{{ _olympus_v2['ssh_key']['pub'] }}"
      loop:
        - "ubuntu"
      tags:
        - ssh-keys

    - name: set correct timezone
      community.general.timezone:
        name: America/New_York
      tags:
        - always

    - name: set correct hostname
      ansible.builtin.hostname:
        name: "{{ inventory_hostname }}"
        use: systemd
      tags:
        - always

    - name: install prerequisites
      ansible.builtin.apt:
        name: "{{ item }}"
        update_cache: true
        autoremove: true
        autoclean: true
      loop: "{{ hostprep_install_dependencies | default([]) }}"
      tags:
        - install-host-prereqs

    - name: collect facts about system services
      service_facts:
      register: services_state

    # TODO: Move this service template so that it points to the correct binary download based on arch.
    - name: place network environment file
      ansible.builtin.template:
        src: "./templates/network-env.j2"
        dest: "/etc/systemd/system/setup-network-environment.service"
        mode: 0644
      notify:
        - reload systemd

    - name: configure modules
      block:
        - name: configure various modules
          community.general.modprobe:
            name: "{{ item }}"
            state: present
          loop: "{{ kernel_modules }}"

        - name: persist module loading at boot
          ansible.builtin.template:
            src: "modprobe.j2"
            dest: "/etc/modprobe.d/{{ ansible_facts['hostname'] }}.conf"
            mode: "0644"

        - name: set sysctl settings
          ansible.builtin.sysctl:
            name: "{{ item['name'] }}"
            value: "{{ item['value'] }}"
            state: present
            sysctl_file: /etc/sysctl.d/10-{{ ansible_facts['hostname'] }}.conf
            sysctl_set: true
          loop:
            - name: net.ipv4.ip_forward
              value: "1"
            - name: net.bridge.bridge-nf-call-iptables
              value: "1"
            - name: net.bridge.bridge-nf-call-ip6tables
              value: "1"
            - name: net.bridge.bridge-nf-call-arptables
              value: "1"
      when:
        - inventory_hostname in groups['app_servers']
      tags:
        - setup-modules

    - name: configure networking
      block:
        - name: install packages only when the apt process is not locked
          ansible.builtin.apt:
            name: "{{ item }}"
            state: present
            update_cache: true
            cache_valid_time: 3600
            autoremove: true
            autoclean: true
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          loop:
            - openvswitch-switch
            - openvswitch-common
            - openvswitch-datapath-dkms
            - openvswitch-switch-dpdk

        # - name: template out conatiner/app interface
        #   ansible.builtin.template:
        #     src: "netplan/config.yaml.j2"
        #     dest: "/etc/netplan/10-vlan-{{ container_vlan }}.yaml"
        #     owner: "root"
        #     group: "root"
        #     mode: "0644"
        #   when:
        #     - netplan_configuration.keys()|length > 0
        #     - inventory_hostname in groups['app_servers']
        #   notify: netplan generate config
        #   vars:
        #     netplan_renderer: "networkd"
        #     netplan_apply: false
        #     netplan_configuration:
        #       network:
        #         version: 2
        #         vlans:
        #           cts:
        #             id: "{{ container_vlan }}"
        #             link: "{{ container_interface }}"
      when:
        - inventory_hostname in groups['app_servers']
        - configure_networking
      tags:
        - setup-network

    - name: place CNI plugins
      block:
        - name: make CNI directories
          ansible.builtin.file:
            path: "{{ item.dir }}"
            owner: "{{ item.owner }}"
            group: "{{ item.group }}"
            mode: "{{ item.mode }}"
            state: directory
          loop:
            - dir: "/opt/cni"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/opt/cni/config"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/opt/cni/bin"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/etc/cni/net.d"
              owner: "root"
              group: "root"
              mode: "0755"

        - name: get and extract CNI binaries
          ansible.builtin.unarchive:
            remote_src: true
            src: "{{ cni['url'] }}v{{ cni['version'] }}/cni-plugins-{{ _system }}-{{ _arch }}-v{{ cni['version'] }}.tgz"
            dest: "/opt/cni/bin"
            mode: "0755"
          tags:
            - skip_ansible_lint

        - name: place templated CNI configuration files in /etc/cni/net.d
          ansible.builtin.template:
            src: "{{ item }}"
            dest: "/etc/cni/net.d/{{ item | basename | splitext | first }}"
            mode: "0644"
          register: _templated_cni
          loop: "{{ lookup('fileglob', './templates/cni/*.conflist.j2', wantlist=True) }}"

        - name: detect cni conf _cni_cnf_files
          ansible.builtin.find:
            paths: "/etc/cni/net.d"
          register: _cni_cnf_files

        # - name: print out detected _cni_cnf_files
        #   ansible.builtin.debug:
        #     var: item['path']
        #   loop: "{{ _cni_cnf_files['files'] }}"

        - name: template out DHCP daemon service and socket for CNI
          ansible.builtin.template:
            src: "{{ item }}"
            dest: "/etc/systemd/system/{{ item | basename | splitext | first }}"
            mode: "0644"
          notify: reload systemd
          loop:
            - "cni/cni-dhcp.service.j2"
            - "cni/cni-dhcp.socket.j2"

        - name: detect if nomad is already installed
          ansible.builtin.command: "which nomad"
          changed_when: false
          failed_when: _nomad_installed.rc not in [0,1]
          register: _nomad_installed

        - name: print out
          ansible.builtin.debug:
            var: _nomad_installed

        - name: symlink templated CNI configuration files to /opt/cni/config for Nomad
          ansible.builtin.file:
            src: "{{ item['path'] }}"
            dest: "/opt/cni/config/{{ item['path'] | basename }}"
            force: true
            state: "link"
          register: _templated_cni_symlink
          loop: "{{ _cni_cnf_files['files'] }}"
          when:
            - _cni_cnf_files.keys() | length > 0
            - _cni_cnf_files['files'] | length > 0

        - name: detect if nomad service is running
          ansible.builtin.systemd:
            name: "nomad"
          register: _nomad_service
          when: _nomad_installed.rc == 0

        - name: print out
          ansible.builtin.debug:
            var: _nomad_service

        - name: restart nomad service if running and cni conf files changed
          ansible.builtin.systemd:
            name: "nomad"
            state: "restarted"
          when:
            - _nomad_service is not skipped
            - _nomad_service['status']['ActiveState'] == "active"
            - _templated_cni.changed or _templated_cni_symlink.changed

        - name: Enable and start cni-dhcp
          ansible.builtin.systemd:
            name: cni-dhcp.socket
            state: started
            enabled: true
      when:
        - inventory_hostname in groups['app_servers']
      tags:
        - install-cni
        - install-podman
        - install-containerd

    - name: install/uninstall containerd
      block:
        - name: install containerd
          ansible.builtin.apt:
            name: "containerd"
            state: present
            update_cache: true
            cache_valid_time: 3600
            autoremove: true
            autoclean: true
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          when:
            - nomad_drivers['containerd']

        - name: uninstall containerd
          ansible.builtin.apt:
            name: "containerd"
            state: absent
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          when:
            - not nomad_drivers['containerd']

        - name: install podman
          ansible.builtin.apt:
            name: "podman"
            state: present
            update_cache: true
            cache_valid_time: 3600
            autoremove: true
            autoclean: true
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          when:
            - nomad_drivers['podman']

        - name: uninstall podman
          ansible.builtin.apt:
            name: "podman"
            state: absent
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          when:
            - not nomad_drivers['podman']

        - name: check for podman network default file.
          ansible.builtin.stat:
            path: "/etc/cni/net.d/87-podman-ptp.conflist"
          register: _podman_default

        - name: remove podman default file if found
          ansible.builtin.file:
            path: "/etc/cni/net.d/87-podman-ptp.conflist"
            state: absent
          when: _podman_default.stat.exists
      when: inventory_hostname in groups['app_servers']
      tags:
        - install-containerd
        - install-podman

    - name: force all notified handlers to run at this point, not waiting for normal sync points
      meta: flush_handlers

    # - name: Enable and start network environment service
    #   ansible.builtin.systemd:
    #     name: setup-network-environment.service
    #     state: started
    #     enabled: true
  roles:
    - role: ddclient_install
      when: install_ddclient
      tags: ddclient-install

    - role: certbot_install
      when: install_certbot
      tags: certbot-install

    - role: ubnt_install
      when: "'ubnt_servers' in group_names"
      tags: ubnt-install

    - role: cloudflare_tunnel
      when: "'cloudflared_nodes' in group_names"
      tags:
        - cloudflared-install
        - cf-install

    - role: traefik_install
      when:
        - "'cloudflared_nodes' in group_names"
        - install_traefik
      tags:
        - traefik-install
        - traefik-setup
        - traefik-cleanup
        - traefik-config

    - role: vault_install
      when: "'vault_servers' in group_names"
      vars:
        vault_certs: "{{ _olympus_v2['vault_hosts'] }}"
        vault_ca: "{{ _olympus_v2['hashi_cas'] }}"
      tags:
        - vault-install
        - vault-certs
        - vault-config
        - hashi-stack

    - role: consul_install
      vars:
        consul_certs: "{{ _olympus_v2['consul_hosts'] }}"
        consul_ca: "{{ _olympus_v2['hashi_cas'] }}"
        consul_encrypt_token: "{{ _olympus_v2['consul_hosts']['consul_secret'] }}"
      tags:
        - consul-install
        - consul-certs
        - consul-config
        - hashi-stack

    - role: nomad_install
      vars:
        nomad_certs: "{{ _olympus_v2['nomad_hosts'] }}"
        nomad_ca: "{{ _olympus_v2['hashi_cas'] }}"
        nomad_encrypt_token: "{{ _olympus_v2['nomad_hosts']['nomad_secret'] }}"
        nomad_log_level: "debug"
      tags:
        - nomad-install
        - nomad-certs
        - nomad-config
        - hashi-stack

  tags:
    - host-prep

- name: vault special tasks
  hosts: vault_servers
  gather_facts: true
  become: true
  handlers:
    - import_tasks: ./handlers/main.yml
  roles:
    - role: vault_init
      tags:
        - vault-init
        - never

    - role: vault_unseal
      tags:
        - vault-unseal
        - never
