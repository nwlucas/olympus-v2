---
- name: fact and backend gathering
  hosts: all
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  tasks:
    - name: Gather facts from ALL hosts (regardless of limit or tags)
      ansible.builtin.setup:
      tags:
        - always

    - name: install packages only when the apt process is not locked
      ansible.builtin.apt:
        name: "{{ item }}"
        state: present
        update_cache: true
        cache_valid_time: 3600
        autoremove: true
        autoclean: true
      register: apt_action
      retries: 100
      until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
      loop:
        - python3-pip
        - net-tools
        - dmidecode
      tags:
        - always

    - name: ensure pip packages are installed
      ansible.builtin.pip:
        name: boto3
        executable: pip3
        umask: "0022"
      tags:
        - always

    - name: gathering terraform outputs
      terraform_backend_info:
        bucket: "{{ aws_bucket }}"
        object: "olympus-v2"
        aws_access_key: "{{ aws_access_key }}"
        aws_secret_access_key: "{{ aws_secret_access_key }}"
        aws_region: "{{ aws_region }}"
      become: false
      register: "olympus_v2"
      check_mode: false
      tags:
        - always
        - terraform_backend_info

    - name: set facts from backend info
      ansible.builtin.set_fact:
        _olympus_v2: "{{ olympus_v2['vars'] }}"
      check_mode: false
      tags:
        - always

    - name: print terraform outputs
      ansible.builtin.copy:
        content: "{{ _olympus_v2 | to_nice_json }}"
        dest: "./backend_info/{{ inventory_hostname }}.json"
        mode: "0744"
      when: print_terraform_backend
      delegate_to: localhost
      become: false
      check_mode: false
      tags:
        - terraform_backend_info

    - name: show terraform outputs
      ansible.builtin.debug:
        var: olympus_v2
      when: terraform_backend_debug is defined and terraform_backend_debug
      check_mode: false
      tags:
        - terraform_backend_info

- name: host preparation
  hosts: all
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  pre_tasks:
    - name: set _arch fact
      ansible.builtin.set_fact:
        _arch: "{%- if ansible_facts['architecture'] == 'aarch64' %}arm64{%- else %}amd64{%- endif %}"
      tags:
        - always

    - name: set _system fact
      ansible.builtin.set_fact:
        _system: "{{ ansible_facts['system']|lower }}"
      tags:
        - always

    - name: add authorized keys
      ansible.posix.authorized_key:
        user: "{{ item }}"
        state: present
        key: "{{ _olympus_v2['ssh_key']['pub'] }}"
      loop:
        - "ubuntu"
      tags:
        - ssh-keys

    - name: set correct timezone
      community.general.timezone:
        name: America/New_York
      tags:
        - always

    - name: set correct hostname
      ansible.builtin.hostname:
        name: "{{ inventory_hostname }}"
        use: systemd
      tags:
        - always

    - name: install prerequisites
      ansible.builtin.apt:
        name: "{{ item }}"
        update_cache: true
        autoremove: true
        autoclean: true
      loop: "{{ hostprep_install_dependencies | default([]) }}"
      tags:
        - install-host-prereqs

    - name: configure modules
      block:
        - name: load required kernel modules
          community.general.modprobe:
            name: "{{ item }}"
            state: present
          loop:
            - "overlay"
            - "br_netfilter"

        - name: remove incorrect modprobe file if found
          ansible.builtin.file:
            path: "/etc/modprobe.d/{{ ansible_facts['hostname'] }}.conf"
            state: absent

        - name: place correct modprobe file
          ansible.builtin.copy:
            content: |
              overlay
              br_netfilter
            dest: "/etc/modules-load.d/{{ ansible_facts['hostname'] }}-containers.conf"
            mode: "0644"

        - name: set sysctl settings
          ansible.builtin.sysctl:
            name: "{{ item['name'] }}"
            value: "{{ item['value'] }}"
            state: present
            sysctl_file: /etc/sysctl.d/50-{{ ansible_facts['hostname'] }}.conf
            sysctl_set: true
            reload: true
          loop:
            - name: net.ipv4.ip_forward
              value: "1"
            - name: net.bridge.bridge-nf-call-iptables
              value: "1"
            - name: net.bridge.bridge-nf-call-ip6tables
              value: "1"
            - name: net.bridge.bridge-nf-call-arptables
              value: "1"
      when:
        - inventory_hostname in groups['app_servers']
      tags:
        - setup-modules

  tasks:
    - name: install ddclient
      ansible.builtin.include_role:
        name: ddclient_install
        apply:
          tags:
            - ddclient-install
      when: install_ddclient
      tags:
        - ddclient-install

    - name: install certbot
      ansible.builtin.include_role:
        name: certbot_install
        apply:
          tags:
            - certbot-install
      when: install_certbot
      tags:
        - certbot-install

    - name: install ubnt
      ansible.builtin.include_role:
        name: ubnt_install
        apply:
          tags:
            - ubnt-install
      when: inventory_hostname in groups['ubnt_servers']
      tags:
        - ubnt-install

    - name: install cloudflare tunnel
      ansible.builtin.include_role:
        name: cloudflare_tunnel
        apply:
          tags:
            - cloudflared-install
            - cf-install
      when: inventory_hostname in groups['cloudflared_nodes']
      tags:
        - cloudflared-install
        - cf-install

    - name: install traefik on host
      ansible.builtin.include_role:
        name: traefik_install
      when:
        - inventory_hostname in groups['cloudflared_nodes']
        - install_traefik
      tags:
        - traefik-install
        - traefik-setup
        - traefik-cleanup
        - traefik-config
  tags:
    - host-prep

- name: install consul cluster
  hosts: consul_servers,app_servers
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  tasks:
    - name: setup consul cluster
      ansible.builtin.include_role:
        name: consul_install
      vars:
        consul_certs: "{{ _olympus_v2['consul_hosts'] }}"
        consul_ca: "{{ _olympus_v2['hashi_cas'] }}"
        consul_encrypt_token: "{{ _olympus_v2['consul_hosts']['consul_secret'] }}"
      tags:
        - consul-install
        - consul-certs
        - consul-config
        - hashi-stack
  tags:
    - consul

- name: install nomad cluster
  hosts: nomad_servers,app_servers
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  pre_tasks:
    - name: place CNI plugins
      block:
        - name: make CNI directories
          ansible.builtin.file:
            path: "{{ item.dir }}"
            owner: "{{ item.owner }}"
            group: "{{ item.group }}"
            mode: "{{ item.mode }}"
            state: directory
          loop:
            - dir: "/opt/cni"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/opt/cni/config"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/opt/cni/bin"
              owner: "root"
              group: "root"
              mode: "0755"
            - dir: "/etc/cni/net.d"
              owner: "root"
              group: "root"
              mode: "0755"

        - name: get and extract CNI binaries
          ansible.builtin.unarchive:
            remote_src: true
            src: "{{ cni['url'] }}v{{ cni['version'] }}/cni-plugins-{{ _system }}-{{ _arch }}-v{{ cni['version'] }}.tgz"
            dest: "/opt/cni/bin"
            mode: "0755"
          tags:
            - skip_ansible_lint

        - name: (1st pass) detect cni conf _cni_cnf_files
          ansible.builtin.find:
            paths: "/etc/cni/net.d"
          register: _cni_cnf_files

        - name: config cni for each app network as needed
          ansible.builtin.include_tasks:
            file: tasks/_special_cni_config.yml
          loop: "{{ app_networks }}"
          loop_control:
            loop_var: app_net
          when:
            - app_networks is defined
            - app_networks|length > 0

        - name: template out DHCP daemon service and socket for CNI
          ansible.builtin.template:
            src: "{{ item }}"
            dest: "/etc/systemd/system/{{ item | basename | splitext | first }}"
            mode: "0644"
          notify: reload systemd
          loop:
            - "cni/cni-dhcp.service.j2"
            - "cni/cni-dhcp.socket.j2"

        - name: Enable and start cni-dhcp
          ansible.builtin.systemd:
            name: cni-dhcp.socket
            state: started
            enabled: true
      when:
        - use_nomad|default(false)
        - inventory_hostname in groups['app_servers']
      tags:
        - install-cni

    - name: install and configure Docker
      block:
        - name: add docker apt key
          ansible.builtin.apt_key:
            id: 9DC858229FC7DD38854AE2D88D81803C0EBFCD88
            url: "https://download.docker.com/linux/ubuntu/gpg"
            keyring: "/usr/share/keyrings/docker-archive-keyring.gpg"
            state: present

        - name: Add Docker repository into sources list using specified filename
          ansible.builtin.apt_repository:
            repo: "deb [arch={{ _arch }} signed-by={{ _keyring }}] https://download.docker.com/linux/ubuntu {{ _release }} stable"
            state: present
            filename: docker-ce
          vars:
            _keyring: "/usr/share/keyrings/docker-archive-keyring.gpg"
            _arch: "{%- if ansible_facts['architecture'] == 'aarch64' %}arm64{%- else %}amd64{%- endif %}"
            _release: "{{ ansible_distribution_release|lower }}"
          tags:
            - skip_ansible_lint

        - name: add kata-containers apt key
          ansible.builtin.apt_key:
            url: "{{ _repo }}:/x86_64:/master/xUbuntu_20.04/Release.key"
            keyring: "/usr/share/keyrings/kata-containers-keyring.gpg"
            state: present
          vars:
            _repo: "http://download.opensuse.org/repositories/home:/katacontainers:/releases"
          when: use_kata_containers
          tags:
            - skip_ansible_lint

        - name: Add kata-containers repository into sources list using specified filename
          ansible.builtin.copy:
            content: "deb [arch=amd64 signed-by={{ _keyring }}] {{ _repo }}:/x86_64/master/xUbuntu_20.04/ /"
            mode: "0644"
            dest: "/etc/apt/sources.list.d/kata-containers.list"
          vars:
            _repo: "http://download.opensuse.org/repositories/home:/katacontainers:/releases"
            _keyring: "/usr/share/keyrings/kata-containers-keyring.gpg"
          when: use_kata_containers
          tags:
            - skip_ansible_lint

        - name: install packages only when the apt process is not locked
          ansible.builtin.apt:
            name: "{{ item }}"
            state: present
            update_cache: true
            cache_valid_time: 3600
            autoremove: true
            autoclean: true
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          loop:
            - docker-ce
            - docker-ce-cli
            - containerd.io
            - cgroupfs-mount
            - aufs-tools

        - name: install packages only when the apt process is not locked
          ansible.builtin.apt:
            name: "{{ item }}"
            state: present
            update_cache: true
            cache_valid_time: 3600
            autoremove: true
            autoclean: true
          register: apt_action
          retries: 100
          until: apt_action is success or ('Failed to lock apt for exclusive operation' not in apt_action.msg and '/var/lib/dpkg/lock' not in apt_action.msg)
          loop:
            - kata-runtime
            - kata-proxy
            - kata-shim
          when: use_kata_containers
      when:
        - inventory_hostname in groups['app_servers']
        - use_docker|default(false)
        - use_nomad|default(false)
      tags:
        - setup-docker
  tasks:
    - name: setup nomad cluster
      ansible.builtin.include_role:
        name: nomad_install
        apply:
          tags:
            - nomad-install
            - nomad-certs
            - nomad-config
            - hashi-stack
      vars:
        nomad_certs: "{{ _olympus_v2['nomad_hosts'] }}"
        nomad_ca: "{{ _olympus_v2['hashi_cas'] }}"
        nomad_encrypt_token: "{{ _olympus_v2['nomad_hosts']['nomad_secret'] }}"
        nomad_log_level: "debug"
      when:
        - use_nomad|default(false)
      tags:
        - nomad-install
        - nomad-certs
        - nomad-config
        - hashi-stack
  tags:
    - nomad

- name: install k3s cluster
  hosts: k3s_cluster
  gather_facts: true
  become: true
  handlers:
    - name: import handlers
      ansible.builtin.import_tasks: ./handlers/main.yml
  pre_tasks:
    - name: Test for raspberry pi /proc/cpuinfo
      ansible.builtin.command: grep -E "Raspberry Pi|BCM2708|BCM2709|BCM2835|BCM2836" /proc/cpuinfo
      register: grep_cpuinfo_raspberrypi
      failed_when: false
      changed_when: false

    - name: Test for raspberry pi /proc/device-tree/model
      ansible.builtin.command: grep -E "Raspberry Pi" /proc/device-tree/model
      register: grep_device_tree_model_raspberrypi
      failed_when: false
      changed_when: false

    - name: Test for Supermicro hosts via dmidecode
      ansible.builtin.shell:
        cmd: set -o pipefail && sudo dmidecode -t system | grep -E "Supermicro"
        executable: /usr/bin/bash
      register: dmidecode_supermicro
      failed_when: false
      changed_when: false

    - name: Test for Dell hosts via dmidecode
      ansible.builtin.shell:
        cmd: set -o pipefail && sudo dmidecode -t system | grep -E "Dell Inc."
        executable: /usr/bin/bash
      register: dmidecode_dell
      failed_when: false
      changed_when: false

    - name: Set raspberry_pi fact to true
      ansible.builtin.set_fact:
        raspberry_pi: true
      when: grep_cpuinfo_raspberrypi.rc == 0 or grep_device_tree_model_raspberrypi.rc == 0

    - name: Set supermicro fact to true
      ansible.builtin.set_fact:
        supermicro: true
      when: dmidecode_supermicro.rc == 0

    - name: Set dell fact to true
      ansible.builtin.set_fact:
        dell: true
      when: dmidecode_dell.rc == 0

    - name: Enable cgroup via boot commandline if not already enabled for Ubuntu on a Raspberry Pi
      ansible.builtin.lineinfile:
        path: /boot/firmware/cmdline.txt
        backrefs: true
        regexp: '^((?!.*\bcgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory\b).*)$'
        line: '\1 cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory'
      notify: reboot
      when: raspberry_pi|default(false)

    - name: Enable cgroup via grub if not already enabled for Ubuntu
      ansible.builtin.lineinfile:
        path: /etc/default/grub
        backrefs: true
        regexp: '^(GRUB_CMDLINE_LINUX=(?!.* systemd\.unified_cgroup_heirarchy=1)\"[^\"]+)(\".*)$'
        line: '\1 systemd.unified_cgroup_heirarchy=1\2'
      notify:
        - update_grub
        - reboot
      when: supermicro|default(false) or dell|default(false)

    - name: Create /etc/systemd/system/user@.service.d directory if it does not exist
      ansible.builtin.file:
        path: /etc/systemd/system/user@.service.d
        state: directory
        mode: "0755"

    - name: enabling cpu, cpuset and i/o delegation
      ansible.builtin.copy:
        content: |
          [Service]
          Delegate=cpu cpuset io memory pids
        dest: "/etc/systemd/system/user@.service.d/delegate.conf"
        mode: "0644"
      notify:
        - systemd-reload
        - reboot

    - name: install wireguard for flannel
      ansible.builtin.apt:
        name: "{{ item }}"
        update_cache: true
        autoremove: true
        autoclean: true
      when: k3s_flannel_backend == "wireguard"
      loop:
        - "wireguard"
  tasks:
    - name: template out env variables
      ansible.builtin.template:
        src: "./templates/k3s-configs/env.j2"
        dest: "/etc/default/k3s"
        mode: "0644"
      vars:
        envs:
          - name: K3S_TOKEN
            value: "{{ _olympus_v2['k3s_cluster']['server_token'] }}"
          - name: K3S_DEBUG
            value: "1"
          - name: K3S_URL
            value: "https://{{ k3s_ha_fqdn }}:{{ k3s_controlplane_port|default('6443') }}"
      notify: restart k3s

    # - name: setup db
    #   block:
    #     - name: Install psycopg2 for ansible to be able to create postgresql users
    #       pip: name=psycopg2-binary
    #     - name: ensure k3s db
    #       community.postgresql.postgresql_db:
    #         name: "{{ vault_db_k3s.db }}"
    #         encoding: "UTF-8"
    #         lc_collate: "en_US.UTF-8"
    #         lc_ctype: "en_US.UTF-8"
    #         template: "template0"
    #         login_host: "{{ vault_db_admin.server }}"
    #         login_user: "{{ vault_db_admin.user }}"
    #         login_password: "{{ vault_db_admin.pass }}"
    #     - name: ensure k3s db user
    #       community.postgresql.postgresql_user:
    #         db: "{{ vault_db_k3s.db }}"
    #         name: "{{ vault_db_k3s.user }}"
    #         password: "{{ vault_db_k3s.pass }}"
    #         comment: "K3S Control Plane User"
    #         login_host: "{{ vault_db_admin.server }}"
    #         login_user: "{{ vault_db_admin.user }}"
    #         login_password: "{{ vault_db_admin.pass }}"
    #   when:
    #     - vault_db_k3s.adapter == "postgres"
    #   delegate_to: localhost
    #   become: false
    #   tags:
    #     - k3s-db
    #     - k3s-install

    - name: setup k3s cluster
      ansible.builtin.include_role:
        name: xanmanning.k3s
        public: true
        apply:
          tags:
            - k3s-install
      vars:
        k3s_become: true
        k3s_skip_validation: false
        k3s_skip_env_checks: false
        k3s_install_hard_links: false
        k3s_use_experimental: true
        k3s_registration_address: "{{ k3s_ha_fqdn }}"
        k3s_service_after: []
        k3s_server_manifests_urls: []
        k3s_server_pod_manifests_urls: []
        k3s_service_env_file: "/etc/default/k3s"
        k3s_server: "{{ lookup('template', './templates/k3s-configs/server.j2') | from_yaml }}"
        k3s_agent: "{{ lookup('template', './templates/k3s-configs/agent.j2') | from_yaml }}"
      when:
        - use_k3s|default(false)
      tags:
        - k3s-install

    - name: register consul service for k3s control plane
      block:
        - name: grab k3s secret with admin token
          ansible.builtin.command:
            argv:
              - kubectl
              - --namespace=kube-system
              - get
              - secret
              - -o=jsonpath='{.items[?(@.metadata.annotations.kubernetes\.io/service-account\.name=="default")].data.token}'
          register: k3s_token_raw
          changed_when: false

        - name: show k3s admin token
          ansible.builtin.set_fact:
            k3s_token: "{{ k3s_token_raw.stdout | b64decode }}"

        - name: test token hitting API/version
          ansible.builtin.uri:
            url: "https://127.0.0.1:6443/version"
            method: "GET"
            return_content: true
            headers:
              Authorization: "Bearer {{ k3s_token }}"
            validate_certs: false
          register: k3s_api_version_output

        - name: set k3s version
          ansible.builtin.set_fact:
            _k3s_version: "{{ k3s_api_version_output['json'] | community.general.json_query('gitVersion') }}"

        - name: template out service registration
          ansible.builtin.template:
            src: "./templates/consul-services/k3s-master.j2"
            dest: "/opt/k3s-service.json"
            mode: "0644"
          register: k3s_consul_svc_tpl
          vars:
            service_name: "{{ k3s_ha_host }}"
          when: k3s_token_raw is succeeded and k3s_token|length > 0

        - name: grab k3s service file from each host
          ansible.builtin.slurp:
            src: "/opt/k3s-service.json"
          register: k3s_consul_svc_file
          when: k3s_consul_svc_tpl is changed

        - name: set k3s version fact
          ansible.builtin.set_fact:
            _k3s_version: "{{ k3s_api_version_output['json'] | community.general.json_query('gitVersion') }}"

        - name: create consul service for k3s server nodes
          ansible.builtin.uri:
            url: "http://127.0.0.1:8500/v1/agent/service/register?replace-existing-checks=true"
            method: "PUT"
            body_format: "json"
            body: "{{ k3s_consul_svc_file['content'] | b64decode }}"
            return_content: true
            status_code: [200, 202]
          register: k3s_service_register
          when: k3s_consul_svc_file is succeeded and k3s_consul_svc_tpl is changed

        - name: show k3s service registration
          ansible.builtin.debug:
            var: k3s_service_register
          when: k3s_service_register is failed

      when:
        - inventory_hostname in groups['k3s_masters']
        - k3s_state|default("installed") != "uninstalled"
      tags:
        - k3s-install
        - k3s-consul-service

    # - name: ensure the proper HA URL is in place for the agent nodes
    #   ansible.builtin.lineinfile:
    #     path: "/etc/default/k3s"
    #     regexp: "^K3S_URL="
    #     line: "K3S_URL=https://{{ k3s_ha_fqdn }}:{{ k3s_controlplane_port|default('6443') }}"
    #   when:
    #     - inventory_hostname in groups['k3s_nodes']
    #     - k3s_state|default("installed") != "uninstalled"
    #   notify: restart k3s
    #   tags:
    #     - k3s-install

    - name: cleanup actions IF K3s state is set to uninstalled
      block:
        - name: create consul service for k3s server nodes
          ansible.builtin.uri:
            url: "http://127.0.0.1:8500/v1/agent/service/deregister/{{ ansible_facts['hostname'] + '-' + service_name }}"
            method: "PUT"
            status_code: [200, 202, 204]
          vars:
            service_name: "{{ k3s_ha_host }}"
          when:
            - inventory_hostname in groups['k3s_masters']

        # - name: ensure k3s db user
        #   community.postgresql.postgresql_user:
        #     db: "{{ vault_db_k3s.db }}"
        #     name: "{{ vault_db_k3s.user }}"
        #     password: "{{ vault_db_k3s.pass }}"
        #     comment: "K3S Control Plane User"
        #     login_host: "{{ vault_db_admin.server }}"
        #     login_user: "{{ vault_db_admin.user }}"
        #     login_password: "{{ vault_db_admin.pass }}"
        #   delegate_to: localhost
        #   become: false
        #   when:
        #     - vault_db_k3s.adapter == "postgres"

        # - name: ensure k3s db
        #   community.postgresql.postgresql_db:
        #     name: "{{ vault_db_k3s.db }}"
        #     encoding: "UTF-8"
        #     lc_collate: "en_US.UTF-8"
        #     lc_ctype: "en_US.UTF-8"
        #     template: "template0"
        #     login_host: "{{ vault_db_admin.server }}"
        #     login_user: "{{ vault_db_admin.user }}"
        #     login_password: "{{ vault_db_admin.pass }}"
        #   delegate_to: localhost
        #   become: false
        #   when:
        #     - vault_db_k3s.adapter == "postgres"
      when:
        - k3s_state is defined
        - k3s_state == "uninstalled"
      tags:
        - k3s-install
        - k3s-consul-service

  tags:
    - k3s
